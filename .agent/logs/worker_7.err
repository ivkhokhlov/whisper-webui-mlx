OpenAI Codex v0.87.0 (research preview)
--------
workdir: /Users/ivankhokhlov/code/mlx-ui/.
model: gpt-5.2-codex
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: xhigh
reasoning summaries: auto
session id: 019be06a-8e30-78b2-bdc3-4a77e36e809b
--------
user
# WORKER AGENT (3-phase harness)

You are the WORKER in a Planner→Worker→Judge pipeline.
Your job: execute `.agent/plan.md` as written, with minimal scope creep.

Non-interactive run (`codex exec`): do not ask questions. If unclear, make the smallest reasonable assumption and log it.

## Stop conditions
- If `.agent/STOP` exists: append “STOP seen” to `.agent/progress.md` and exit.
- If `.agent/DONE` exists: append “DONE seen” to `.agent/progress.md` and exit.

## Inputs you MUST read
- `.agent/PROJECT.md` (authoritative; includes TEST_CMD/LINT_CMD)
- `AGENTS.md` (repo rules)
- `docs/spec.md`
- `docs/dev.md` (if exists)
- `.agent/plan.md`
- `.agent/judge_feedback.md` (if present)
- `.agent/queue.md`
- `.agent/progress.md`
- `.agent/state.md`

## Core rules
- Implement ONLY the single task in `.agent/plan.md`.
- Do NOT mark tasks done in `.agent/queue.md` (Judge owns that).
- Prefer small, coherent changes. No big refactors unless the task explicitly requires it.
- All network-dependent logic must be best-effort with timeouts and exception handling.
- Bind server to localhost only.
- Queue must be strictly sequential: one job at a time; no parallel transcription runs.
- Never print secrets. If you need to log config, mask tokens.

## Logging
Ensure `.agent/logs/` exists.
Write/overwrite `.agent/worker_report.md` with:
- task id/title
- what changed (bullets)
- files changed (list)
- commands run + result (pass/fail)
- if failure: include a short excerpt of the error and what you think is next

Also append a WORKER entry to `.agent/progress.md` with:
- timestamp
- CODEX_ITERATION
- task id
- test + lint status summary

## Verification (must do)
Run TEST_CMD (from PROJECT.md). Capture output into a file.

Tip: to preserve exit codes with tee:
- use: `bash -lc "set -o pipefail; <TEST_CMD> 2>&1 | tee .agent/logs/test_${CODEX_ITERATION}.log"`

If LINT_CMD is set and command exists, run similarly and capture output:
- `.agent/logs/lint_${CODEX_ITERATION}.log`

If tests fail and you cannot fix quickly:
- write/update `.agent/BLOCKED.md` with exact errors + next steps
- exit (do not thrash)

## Output
Print a short summary:
- what you implemented
- test status
- pointers to worker_report + log files

mcp startup: no servers

thinking
**Clarifying repository boundaries and workflow**

Planning to follow instructions strictly by reading files within the repo only, using shell commands like ripgrep and cat, while ensuring to check for STOP/DONE signals in the content.
exec
/bin/zsh -lc ls in /Users/ivankhokhlov/code/mlx-ui succeeded in 60ms:
AGENTS.md
Makefile
README.md
data
docs
mlx_ui
poetry.lock
pyproject.toml
scripts
tests

thinking
**Checking for .agent presence**
exec
/bin/zsh -lc 'ls -a' in /Users/ivankhokhlov/code/mlx-ui succeeded in 85ms:
.
..
.DS_Store
.agent
.git
.gitignore
.pytest_cache
.python-version
.ruff_cache
.venv
AGENTS.md
Makefile
README.md
data
docs
mlx_ui
poetry.lock
pyproject.toml
scripts
tests
exec
/bin/zsh -lc 'ls .agent' in /Users/ivankhokhlov/code/mlx-ui succeeded in 72ms:
BLOCKED.md
PROJECT.md
judge.md
judge_feedback.md
last_judge.txt
last_planner.txt
last_worker.txt
logs
plan.md
planner.md
progress.md
queue.md
state.md
worker.md
worker_report.md

thinking
**Confirming no stop signals**

No stop or done signals detected; proceeding to read inputs using standard input.
exec
/bin/zsh -lc 'cat AGENTS.md' in /Users/ivankhokhlov/code/mlx-ui succeeded in 91ms:
# AGENTS.md

This repo is developed using an autonomous loop (Planner → Worker → Judge) driven by `codex exec`.
Because each run starts with limited context, **durable state MUST be stored in `.agent/`**.

## Source of truth
- `.agent/PROJECT.md` — high-level constraints and commands
- `docs/spec.md` — product requirements
- `.agent/queue.md` — backlog (Judge marks tasks done)

## How to run
- Tests: `make test`
- Lint: `make lint`
- Format: `make fmt`
- Run server: `make run`

If Makefile does not exist yet, create it during bootstrap.

## Safety / security rules
- Never touch anything outside the repo root.

thinking
**Planning to read project metadata**
exec
/bin/zsh -lc 'cat .agent/PROJECT.md' in /Users/ivankhokhlov/code/mlx-ui succeeded in 76ms:
# PROJECT (single source of truth for the harness)

PROJECT_NAME: "whisper-webui-mlx"
REPO_KIND: "new repo"

## Brief (1–2 paragraphs)
Build a **local macOS app for Apple Silicon (M1+)** that exposes a **Web UI on localhost** to manage **offline transcription** of audio/video files using the engine from `whisper-turbo-mlx` via its CLI `wtm` (MLX). https://github.com/JosefAlbers/whisper-turbo-mlx

Turn the current console workflow (e.g. `wtm ... --any_lang=True`) into a tool that is easy to deploy on another M1+ Mac with **one command**, supports **batch uploads**, a **strictly sequential** processing queue (no parallelism), **job history + downloads**, optional **Telegram delivery of TXT results**, and (in the final phase) **live recording + chunked transcription**.

## Primary stack
Python 3.11+
- Backend/Web: FastAPI + Uvicorn
- Templates/UI: Jinja2 + minimal vanilla JS (no external CDNs; must work offline)
- Storage: SQLite (jobs metadata) + local filesystem (uploads/results/logs)
- Worker: a single sequential worker consuming a persistent queue
- Transcription: subprocess call to `wtm` with explicit language selection

## Commands (choose defaults if unknown)
TEST_CMD: "make test"

thinking
**Identifying missing documentation file**
exec
/bin/zsh -lc 'cat docs/spec.md' in /Users/ivankhokhlov/code/mlx-ui succeeded in 57ms:
# Spec — whisper-webui-mlx

## Goal
Create a **local-only** tool for **macOS Apple Silicon (M1+)** that provides a **Web UI on localhost** to manage **local transcription** of audio/video files using the engine from `whisper-turbo-mlx` via its CLI `wtm` (MLX).

The tool must be easy to deploy to another M1+ Mac (one command), and after the initial setup + model download it must work **fully offline**.

## MVP: Batch file transcription via Web UI
### UX
- User opens Web UI (localhost).
- User uploads **one or multiple files** (audio or video).
- User selects the **language manually** (no auto-detect).
- Files are placed into a **queue** and processed **strictly sequentially** (no parallel jobs).
- UI has:
  - Queue view: current job + pending jobs
  - History view: completed jobs + access to results

### Processing rules
- One worker processes one job at a time (sequential).
- Keep the ML model “warm” behavior in mind: avoid parallelism and avoid reinitialization churn.

thinking
**Checking for docs/dev.md**

I'm listing the docs directory to see if a dev.md file exists for reference.
exec
/bin/zsh -lc 'ls docs' in /Users/ivankhokhlov/code/mlx-ui succeeded in 55ms:
dev.md
spec.md
tree.md
exec
/bin/zsh -lc 'cat docs/dev.md' in /Users/ivankhokhlov/code/mlx-ui succeeded in 65ms:
# Developer guide

## Requirements (target)
- macOS Apple Silicon (M1+)
- Python 3.11+
- Homebrew (for system deps like ffmpeg)

## Quick start (eventual)
```bash
./scripts/setup_and_run.sh
```

## Manual dev loop
```bash
poetry install --with dev

make test
make run
```


thinking
**Defining agent file requirements**

Planning the creation of essential agent files—plan, queue, progress, state, judge_feedback—and adding checks to halt if STOP or DONE files exist, validating file existence via shell commands.
exec
/bin/zsh -lc 'ls .agent/STOP .agent/DONE 2>/dev/null' in /Users/ivankhokhlov/code/mlx-ui exited 1 in 65ms:


thinking
**Reading key state files**

I'm reading the plan, judgement feedback, queue, progress, and state files to gather essential context for the next steps.
exec
/bin/zsh -lc 'cat .agent/plan.md' in /Users/ivankhokhlov/code/mlx-ui succeeded in 68ms:
# Plan

Task: WUI-012 — Sequential worker + fake transcriber
Acceptance: single worker processes jobs strictly one-at-a-time; tests do not require ML model

Assumptions:
- A background loop can run in-process (thread or asyncio task) on app startup to poll queued jobs.
- Fake transcriber can write a stub TXT result under `data/results/<job_id>/result.txt` without calling `wtm`.

Implementation steps:
- Add DB helpers to fetch the next queued job and update job status (queued/running/done/failed) atomically.
- Implement a single worker loop that polls for queued jobs, marks one running, runs the fake transcriber, then marks done or failed.
- Guard the worker with a single global lock or singleton to prevent concurrent execution.
- Create a small fake transcriber module that writes a deterministic TXT result for each job.
- Wire the worker startup into FastAPI startup so it begins processing uploads automatically.
- Add tests that enqueue multiple jobs and assert they complete sequentially without parallel execution or real ML model calls.
- Update `docs/tree.md` if new modules or data directories are added.
- Record test/lint results in `.agent/worker_report.md`.

Files likely to touch:
exec
/bin/zsh -lc 'cat .agent/judge_feedback.md' in /Users/ivankhokhlov/code/mlx-ui succeeded in 70ms:
PASS

Notes:
- `make test` passes (rerun).
- `make lint` not rerun; worker reported pass.
- Acceptance criteria met for WUI-011.

thinking
**Identifying required components**

I see that implementing queue, progress tracking, and state management are necessary next steps.
exec
/bin/zsh -lc 'cat .agent/queue.md' in /Users/ivankhokhlov/code/mlx-ui succeeded in 58ms:
# Queue

## Now
- [ ] WUI-012 — Sequential worker + fake transcriber (acceptance: single worker processes jobs strictly one-at-a-time; tests do not require ML model)

## Next

## Later
- [ ] WUI-020 — Integrate `wtm` CLI transcription (acceptance: for a job, system runs `wtm` with chosen language and produces at least `.txt` in `data/results/<job_id>/`)
- [ ] WUI-021 — Results browsing + download (acceptance: history page shows per-job links to view/download TXT and other generated formats)
- [ ] WUI-030 — One-command setup & run on M1+ macOS (acceptance: `./scripts/setup_and_run.sh` installs deps, downloads models, starts server, and opens browser)
- [ ] WUI-040 — Telegram delivery (acceptance: when env vars are set, send TXT + message; failures do not break pipeline; secrets masked)
- [ ] WUI-050 — Update check at startup (acceptance: if online, checks for updates with timeout; if offline, no errors and app works)
- [ ] WUI-090 — Live mode skeleton (acceptance: UI has “Live” page stub + technical plan in docs; no implementation yet)

## Done
- [x] WUI-011 — Persistent job store (SQLite) (acceptance: jobs survive server restart; history page lists prior jobs)
- [x] WUI-010 — Upload endpoint + local storage (acceptance: user can upload multiple files; files saved under `data/uploads/` and jobs are created)
- [x] WUI-002 — Minimal WebUI shell (acceptance: localhost page renders with tabs: Queue + History, using server-side templates)
- [x] WUI-001A — Fix pytest availability for `make test` (acceptance: `make test` succeeds in a clean env by ensuring pytest is installed or Makefile uses Poetry/venv)
exec
/bin/zsh -lc 'cat .agent/progress.md' in /Users/ivankhokhlov/code/mlx-ui succeeded in 86ms:
# Progress Log (append-only)

- 2026-01-21T09:49:08 — INIT — Harness files created. Next: run the 3-phase loop.
- 2026-01-21T10:33:08Z — PLANNER — CODEX_ITERATION=1 — selected WUI-001 (Bootstrap repo skeleton). Queue unchanged.
- 2026-01-21T10:37:23Z — WORKER — CODEX_ITERATION=1 — WUI-001 — tests: failed (pytest missing); lint: not run
- 2026-01-21T10:39:53Z — JUDGE — CODEX_ITERATION=1 — NEEDS_WORK — make test failed (pytest missing in venv); tests: failed (make test)
- 2026-01-21T10:42:24Z — PLANNER — CODEX_ITERATION=2 — selected WUI-001A (Fix pytest availability for `make test`). Queue updated (format fix for WUI-001A).
- 2026-01-21T10:49:11Z — WORKER — CODEX_ITERATION=2 — WUI-001A — tests: passed (make test); lint: passed (make lint)
- 2026-01-21T10:52:47Z — JUDGE — CODEX_ITERATION=2 — PASS — acceptance met; tests: passed (make test); lint: passed (make lint)
- 2026-01-21T10:56:28Z — PLANNER — CODEX_ITERATION=3 — selected WUI-001 (Bootstrap repo skeleton). Queue unchanged.
- 2026-01-21T10:58:25Z — WORKER — CODEX_ITERATION=3 — WUI-001 — tests: passed (make test); lint: passed (make lint)
- 2026-01-21T11:01:51Z — JUDGE — CODEX_ITERATION=3 — PASS — acceptance met; tests: passed (make test); lint: passed (make lint)
- 2026-01-21T11:05:17Z — PLANNER — CODEX_ITERATION=4 — selected WUI-002 (Minimal WebUI shell). Queue unchanged.
- 2026-01-21T11:11:48Z — WORKER — CODEX_ITERATION=4 — WUI-002 — tests: passed (make test); lint: passed (make lint)
- 2026-01-21T11:16:13Z — JUDGE — CODEX_ITERATION=4 — PASS — acceptance met; tests: passed (make test); lint: passed (make lint)
- 2026-01-21T11:23:27Z — PLANNER — CODEX_ITERATION=5 — selected WUI-010 (Upload endpoint + local storage). Queue updated (moved WUI-010 to Now).
- 2026-01-21T11:33:16Z — WORKER — CODEX_ITERATION=5 — WUI-010 — tests: passed (make test); lint: passed (make lint)
- 2026-01-21T11:37:21Z — JUDGE — CODEX_ITERATION=5 — PASS — acceptance met; tests: passed (make test); lint: not rerun (worker reported pass)
- 2026-01-21T11:41:40Z — PLANNER — CODEX_ITERATION=6 — selected WUI-011 (Persistent job store (SQLite)). Queue updated (moved WUI-011 to Now).
- 2026-01-21T11:48:06Z — WORKER — CODEX_ITERATION=6 — WUI-011 — tests: passed (make test); lint: passed (make lint)

thinking
**Checking state documentation**
exec
/bin/zsh -lc 'cat .agent/state.md' in /Users/ivankhokhlov/code/mlx-ui succeeded in 70ms:
last_task_id: WUI-011
last_verdict: PASS
consecutive_failures: 0

thinking
**Inspecting core modules**
exec
/bin/zsh -lc 'rg --files mlx_ui' in /Users/ivankhokhlov/code/mlx-ui succeeded in 85ms:
mlx_ui/templates/index.html
mlx_ui/app.py
mlx_ui/__init__.py
mlx_ui/db.py

thinking
**Opening key modules**
exec
/bin/zsh -lc "sed -n '1,200p' mlx_ui/app.py" in /Users/ivankhokhlov/code/mlx-ui succeeded in 87ms:
from datetime import datetime, timezone
from pathlib import Path
import shutil
from uuid import uuid4

from fastapi import FastAPI, File, Request, UploadFile
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates

from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs

app = FastAPI(title="Whisper WebUI (MLX)")
templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
BASE_DIR = Path(__file__).resolve().parent.parent
DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
app.state.uploads_dir = DEFAULT_UPLOADS_DIR
app.state.db_path = DEFAULT_DB_PATH



thinking
**Noting missing database module**
exec
/bin/zsh -lc "sed -n '1,200p' mlx_ui/db.py" in /Users/ivankhokhlov/code/mlx-ui succeeded in 91ms:
from dataclasses import dataclass
from pathlib import Path
import sqlite3


@dataclass
class JobRecord:
    id: str
    filename: str
    status: str
    created_at: str
    upload_path: str


SCHEMA = """
CREATE TABLE IF NOT EXISTS jobs (
    id TEXT PRIMARY KEY,
    filename TEXT NOT NULL,
    status TEXT NOT NULL,
    created_at TEXT NOT NULL,

thinking
**Checking for existing tests**

I'm inspecting the test files to see if tests already cover the worker and fake transcriber components before proceeding with new implementations.
exec
/bin/zsh -lc 'rg --files tests' in /Users/ivankhokhlov/code/mlx-ui succeeded in 82ms:
tests/test_app.py
tests/__init__.py
exec
/bin/zsh -lc "sed -n '1,200p' tests/test_app.py" in /Users/ivankhokhlov/code/mlx-ui succeeded in 68ms:
from pathlib import Path

from fastapi.testclient import TestClient

from mlx_ui.app import app
from mlx_ui.db import list_jobs


def _configure_app(tmp_path: Path) -> None:
    app.state.uploads_dir = tmp_path / "uploads"
    app.state.db_path = tmp_path / "jobs.db"


def test_root_ok(tmp_path: Path) -> None:
    _configure_app(tmp_path)
    with TestClient(app) as client:
        response = client.get("/")

    assert response.status_code == 200
    assert response.headers["content-type"].startswith("text/html")

thinking
**Planning worker tests**

I'm identifying a need to add or update tests for workers and will review docs/tree.md to inform this.
exec
/bin/zsh -lc 'cat docs/tree.md' in /Users/ivankhokhlov/code/mlx-ui succeeded in 62ms:
# Project tree (curated)

This file is a human-maintained map of the repo. Update when structure changes.

## Current (bootstrap)
- `.agent/` — durable loop state + prompts
- `data/` — runtime uploads/results/logs/jobs.db (created on demand)
- `docs/` — spec + dev notes + this tree map
- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
- `mlx_ui/templates/` — Jinja2 templates
- `scripts/` — loop scripts + setup/run scripts (to be implemented)
- `tests/` — pytest suite
- `Makefile` — dev commands
- `pyproject.toml` — dependencies and tooling
- `README.md` — repo overview


thinking
**Planning atomic job status updates**

Evaluating SQLite's ability to atomically fetch and update the next queued job without altering the schema; considering adding error or timestamp columns but the plan and tests may not require schema changes.
thinking
**Designing fake transcriber and worker**

Planning a fake transcriber module that writes deterministic results per job and a singleton background worker using threading to process jobs sequentially with a global lock. Tests will ensure sequential processing by verifying job status transitions and may inject concurrency checks via the transcriber.
exec
/bin/zsh -lc 'cat pyproject.toml' in /Users/ivankhokhlov/code/mlx-ui succeeded in 58ms:
[tool.poetry]
name = "mlx-ui"
version = "0.1.0"
description = ""
authors = ["Ivan Khokhlov <ivan.s.khokhlov@rt.ru>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.11"
fastapi = "^0.115.6"
jinja2 = "^3.1.4"
uvicorn = "^0.32.1"
python-multipart = "^0.0.21"

[tool.poetry.group.dev.dependencies]
httpx = "^0.28.1"
pytest = "^8.3.4"
ruff = "^0.14.13"



thinking
**Checking Makefile for test commands**
exec
/bin/zsh -lc 'cat Makefile' in /Users/ivankhokhlov/code/mlx-ui succeeded in 67ms:
PYTHON ?= python
POETRY ?= poetry
POETRY_RUN ?= $(POETRY) run

.PHONY: test lint fmt run

test:
	$(POETRY_RUN) pytest

lint:
	$(POETRY_RUN) ruff check .

fmt:
	$(POETRY_RUN) ruff format .

run:
	$(POETRY_RUN) uvicorn mlx_ui.app:app --host 127.0.0.1 --port 8000


thinking
**Outlining worker design and DB atomic claim**

I'm planning a Worker class in `mlx_ui/worker.py` to process jobs sequentially with stoppable threading, ensuring tests can terminate quickly. For job claiming in `db.py`, I'll implement an atomic function using SQLite's `BEGIN IMMEDIATE` transaction to securely pick and update the next queued job without race conditions.
thinking
**Designing atomic DB claim and Worker loop**

Decided to use a dedicated SQLite connection with `isolation_level=None` for manual transaction control and run `BEGIN IMMEDIATE` to atomically claim jobs without transaction conflicts. The Worker class will run a stoppable thread loop that claims jobs, processes them via an injectable transcribe function, updates job status, and sleeps with interruptible waits for responsiveness and test control. I also scoped worker start logic to prevent concurrency and planned app startup integration with optional worker disabling for tests.
exec
/bin/zsh -lc 'echo ${CODEX_ITERATION:-unknown}' in /Users/ivankhokhlov/code/mlx-ui succeeded in 80ms:
7


thinking
**Planning results directory and worker setup**

I'm designing to add a results directory path to app state with helper functions to get and ensure it exists. The startup will initialize the DB and start the worker only if enabled, defaulting worker_enabled to True, with tests disabling it as needed.
thinking
**Designing robust sequential processing test**

I'm planning a test using a thread-safe recording transcriber to verify the worker processes jobs strictly sequentially without concurrency, asserting job status order matches insertion order and using a helper to wait for job completion. I'll also add worker stop support for clean test teardown to avoid interference.
exec
/bin/zsh -lc "sed -n '1,200p' mlx_ui/templates/index.html" in /Users/ivankhokhlov/code/mlx-ui succeeded in 84ms:
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Whisper WebUI (MLX)</title>
    <style>
      :root {
        --ink: #1d1b16;
        --ink-muted: #5d5d5d;
        --panel: #fff7ee;
        --accent: #d46a3e;
        --accent-strong: #8b3f1f;
        --shadow: 0 24px 60px rgba(29, 27, 22, 0.12);
      }

      * {
        box-sizing: border-box;
      }



thinking
**Planning job claim implementation**

I'm preparing to implement `claim_next_job` with explicit transaction handling to atomically select and update a queued job to running, using manual connection and row factory for `JobRecord` compatibility. Next, I'll ensure the returned record reflects the updated status.
thinking
**Detailing job claiming and worker design**

Planning to return claimed jobs with updated status "running" and ensure proper commit/rollback with `BEGIN IMMEDIATE`. Preparing a Worker class with single-threaded polling and sequential job processing guarded by a global lock, plus a FakeTranscriber for tests. Designing robust tests to confirm strict sequential job processing and status updates while managing worker lifecycle and results directories.
file update
M /Users/ivankhokhlov/code/mlx-ui/./mlx_ui/db.py
@@ -61 +61,52 @@
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
apply_patch(auto_approved=true) exited 0 in 17ms:
Success. Updated the following files:
M mlx_ui/db.py
file update:
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()

file update:
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()


thinking
**Adding transcriber module**

I'm creating a new `mlx_ui/transcriber.py` file and ensuring that `claim_next_job` returns the running status as expected.
file update
A /Users/ivankhokhlov/code/mlx-ui/./mlx_ui/transcriber.py
from pathlib import Path
from typing import Protocol

from mlx_ui.db import JobRecord


class Transcriber(Protocol):
    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
        raise NotImplementedError


class FakeTranscriber:
    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
        results_dir = Path(results_dir)
        job_dir = results_dir / job.id
        job_dir.mkdir(parents=True, exist_ok=True)
        result_path = job_dir / "result.txt"
        content = f"Fake transcript for {job.filename} ({job.id})\n"
        result_path.write_text(content, encoding="utf-8")
        return result_path
apply_patch(auto_approved=true) exited 0 in 23ms:
Success. Updated the following files:
A mlx_ui/transcriber.py
file update:
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path

file update:
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path


thinking
**Adding worker module**
file update
A /Users/ivankhokhlov/code/mlx-ui/./mlx_ui/worker.py
from __future__ import annotations

import logging
from pathlib import Path
import threading

from mlx_ui.db import claim_next_job, update_job_status
from mlx_ui.transcriber import FakeTranscriber, Transcriber

logger = logging.getLogger(__name__)

_worker_lock = threading.Lock()
_worker_instance: Worker | None = None


class Worker:
    def __init__(
        self,
        db_path: Path,
        results_dir: Path,
        poll_interval: float = 0.5,
        transcriber: Transcriber | None = None,
    ) -> None:
        self.db_path = Path(db_path)
        self.results_dir = Path(results_dir)
        self.poll_interval = poll_interval
        self.transcriber = transcriber or FakeTranscriber()
        self._stop_event = threading.Event()
        self._thread: threading.Thread | None = None

    def start(self) -> None:
        if self.is_running():
            return
        self._stop_event.clear()
        self._thread = threading.Thread(
            target=self._run_loop,
            name="mlx-ui-worker",
            daemon=True,
        )
        self._thread.start()

    def stop(self, timeout: float | None = None) -> None:
        self._stop_event.set()
        thread = self._thread
        if thread is not None:
            thread.join(timeout=timeout)

    def is_running(self) -> bool:
        return self._thread is not None and self._thread.is_alive()

    def _run_loop(self) -> None:
        while not self._stop_event.is_set():
            processed = self.run_once()
            if not processed:
                self._stop_event.wait(self.poll_interval)

    def run_once(self) -> bool:
        job = claim_next_job(self.db_path)
        if job is None:
            return False
        try:
            self.transcriber.transcribe(job, self.results_dir)
        except Exception:
            logger.exception("Worker failed to transcribe job %s", job.id)
            update_job_status(self.db_path, job.id, "failed")
            return True
        update_job_status(self.db_path, job.id, "done")
        return True


def start_worker(
    db_path: Path,
    results_dir: Path,
    poll_interval: float = 0.5,
    transcriber: Transcriber | None = None,
) -> Worker:
    global _worker_instance
    with _worker_lock:
        if _worker_instance and _worker_instance.is_running():
            return _worker_instance
        _worker_instance = Worker(
            db_path=db_path,
            results_dir=results_dir,
            poll_interval=poll_interval,
            transcriber=transcriber,
        )
        _worker_instance.start()
        return _worker_instance


def stop_worker(timeout: float | None = None) -> None:
    global _worker_instance
    with _worker_lock:
        if not _worker_instance:
            return
        _worker_instance.stop(timeout=timeout)
        _worker_instance = None
apply_patch(auto_approved=true) exited 0 in 29ms:
Success. Updated the following files:
A mlx_ui/worker.py
file update:
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None

file update:
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None


thinking
**Adding worker setup and results dir to app**

I'm modifying app.py to add a results directory in app state, enable the worker by default, and start the worker on startup after initializing the database. I’ll ensure the results directory exists before starting the worker and adjust imports accordingly.
file update
M /Users/ivankhokhlov/code/mlx-ui/./mlx_ui/app.py
@@ -10,2 +10,3 @@
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
@@ -15,5 +16,8 @@
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
@@ -23,2 +27,4 @@
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
@@ -37,2 +43,6 @@
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
apply_patch(auto_approved=true) exited 0 in 43ms:
Success. Updated the following files:
M mlx_ui/app.py
file update:
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None

file update:
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None


thinking
**Considering results_dir setup**

I'm weighing adding a simple `ensure_results_dir()` for clarity and robustness, but it's not strictly required since the worker's FakeTranscriber creates job-specific directories. Next, I'll update tests to disable the worker and explicitly set `results_dir`.
file update
M /Users/ivankhokhlov/code/mlx-ui/./tests/test_app.py
@@ -10,3 +10,5 @@
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
apply_patch(auto_approved=true) exited 0 in 37ms:
Success. Updated the following files:
M tests/test_app.py
file update:
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:

file update:
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:



thinking
**Starting worker test setup**

I'm creating a new test file with a thread-safe fake transcriber class to detect concurrency and write dummy results, along with a helper to insert jobs and prepare for end-to-end worker testing.
thinking
**Adding sequential job processing test**

I'm implementing a test that inserts two jobs with distinct creation times, starts the worker with a recording transcriber, waits for both jobs to complete, then asserts no concurrency occurred and results exist in correct order. The stop_worker call ensures clean worker shutdown.
file update
A /Users/ivankhokhlov/code/mlx-ui/./tests/test_worker.py
from datetime import datetime, timedelta, timezone
from pathlib import Path
import threading
import time

from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
from mlx_ui.worker import start_worker, stop_worker


class RecordingTranscriber:
    def __init__(self) -> None:
        self._lock = threading.Lock()
        self._active = False
        self.concurrent_detected = False
        self.seen: list[str] = []

    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
        with self._lock:
            if self._active:
                self.concurrent_detected = True
            self._active = True
            self.seen.append(job.id)

        time.sleep(0.05)

        job_dir = Path(results_dir) / job.id
        job_dir.mkdir(parents=True, exist_ok=True)
        result_path = job_dir / "result.txt"
        content = f"Fake transcript for {job.filename} ({job.id})\n"
        result_path.write_text(content, encoding="utf-8")

        with self._lock:
            self._active = False
        return result_path


def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
    job_dir = uploads_dir / job_id
    job_dir.mkdir(parents=True, exist_ok=True)
    upload_path = job_dir / filename
    upload_path.write_text("data", encoding="utf-8")
    return JobRecord(
        id=job_id,
        filename=filename,
        status="queued",
        created_at=created_at,
        upload_path=str(upload_path),
    )


def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
    deadline = time.monotonic() + timeout
    while time.monotonic() < deadline:
        jobs = list_jobs(db_path)
        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
            return jobs
        time.sleep(0.01)
    raise AssertionError("Timed out waiting for jobs to complete.")


def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
    db_path = tmp_path / "jobs.db"
    uploads_dir = tmp_path / "uploads"
    results_dir = tmp_path / "results"
    init_db(db_path)
    uploads_dir.mkdir(parents=True, exist_ok=True)

    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
    job1 = _make_job(
        "job1",
        "alpha.txt",
        base_time.isoformat(timespec="seconds"),
        uploads_dir,
    )
    job2 = _make_job(
        "job2",
        "beta.txt",
        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
        uploads_dir,
    )
    insert_job(db_path, job1)
    insert_job(db_path, job2)

    transcriber = RecordingTranscriber()
    worker = start_worker(
        db_path,
        results_dir,
        poll_interval=0.01,
        transcriber=transcriber,
    )
    try:
        jobs = _wait_for_jobs(db_path, expected_count=2)
    finally:
        stop_worker(timeout=1)

    assert transcriber.concurrent_detected is False
    assert transcriber.seen == [job1.id, job2.id]
    for job in jobs:
        result_path = results_dir / job.id / "result.txt"
        assert result_path.is_file()
        assert job.status == "done"
        assert job.filename in result_path.read_text(encoding="utf-8")
apply_patch(auto_approved=true) exited 0 in 54ms:
Success. Updated the following files:
A tests/test_worker.py
file update:
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..2758b55ad96b03f0c3fce4812a50dfa317cf5a1a
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    worker = start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")

file update:
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..2758b55ad96b03f0c3fce4812a50dfa317cf5a1a
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    worker = start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")


thinking
**Removing unused variable in test**
file update
M /Users/ivankhokhlov/code/mlx-ui/./tests/test_worker.py
@@ -84,3 +84,3 @@
     transcriber = RecordingTranscriber()
-    worker = start_worker(
+    start_worker(
         db_path,
apply_patch(auto_approved=true) exited 0 in 57ms:
Success. Updated the following files:
M tests/test_worker.py
file update:
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")

file update:
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")


thinking
**Updating documentation for new modules**

I'm adding the new files mlx_ui/worker.py, mlx_ui/transcriber.py, and tests/test_worker.py to docs/tree.md and updating the mlx_ui section to mention the new modules.
file update
M /Users/ivankhokhlov/code/mlx-ui/./docs/tree.md
@@ -8,6 +8,6 @@
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
apply_patch(auto_approved=true) exited 0 in 58ms:
Success. Updated the following files:
M docs/tree.md
file update:
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")

file update:
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")



thinking
**Reviewing import and startup order**

The get_results_dir function is used safely before its definition in app.py, so no import reorder is needed. The results directory creation is handled transitively via job_dir, avoiding direct creation on startup. Test setup properly disables the worker before TestClient runs, ensuring no unintended worker startup during tests.
thinking
**Verifying worker lifecycle and global state handling**

The worker start/stop logic in tests safely avoids leftover workers, and disabling on each test setup prevents cross-test interference. Although app state flags persist globally, repeated setup calls keep them consistent, so no reset is needed. No updates are required to __init__.py files for exports.
exec
/bin/zsh -lc 'cat mlx_ui/__init__.py' in /Users/ivankhokhlov/code/mlx-ui succeeded in 61ms:

file update:
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")


thinking
**Running tests with logging**
exec
/bin/zsh -lc 'bash -lc "set -o pipefail; make test 2>&1 | tee .agent/logs/test_7.log"' in /Users/ivankhokhlov/code/mlx-ui succeeded in 1.31s:
poetry run pytest
============================= test session starts ==============================
platform darwin -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0
rootdir: /Users/ivankhokhlov/code/mlx-ui
configfile: pyproject.toml
plugins: anyio-4.12.1
collected 4 items

tests/test_app.py ...                                                    [ 75%]
tests/test_worker.py .                                                   [100%]

=============================== warnings summary ===============================
mlx_ui/app.py:25
  /Users/ivankhokhlov/code/mlx-ui/mlx_ui/app.py:25: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("startup")
file update:
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")


thinking
**Running lint check**
exec
/bin/zsh -lc 'bash -lc "set -o pipefail; make lint 2>&1 | tee .agent/logs/lint_7.log"' in /Users/ivankhokhlov/code/mlx-ui succeeded in 643ms:
poetry run ruff check .
All checks passed!
file update:
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")


thinking
**Planning worker report update**

I'm outlining the new worker_report.md format to include task details, change summary, files modified, commands run with results, and logs path, while appending progress info with timestamps and iteration data to .agent/progress.md before overwriting worker_report.md.
exec
/bin/zsh -lc "cat <<'EOF' > .agent/worker_report.md
# Worker Report

Task: WUI-012 — Sequential worker + fake transcriber

What changed:
- Added an atomic job-claim API plus status updates for queued/running/done/failed.
- Implemented a single background worker and fake transcriber that writes deterministic TXT output.
- Wired worker startup into app startup and added a sequential worker test.
- Updated existing app tests to keep the worker disabled during upload checks.
- Updated docs tree to reflect new modules/tests.

Files changed:
- mlx_ui/app.py
- mlx_ui/db.py
- mlx_ui/transcriber.py
- mlx_ui/worker.py
- tests/test_app.py
- tests/test_worker.py
- docs/tree.md

Commands run:
- make test (pass)
- make lint (pass)
EOF" in /Users/ivankhokhlov/code/mlx-ui succeeded in 86ms:

file update:
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")


thinking
**Appending timestamped progress entry**

I'm preparing to append a progress update to .agent/progress.md with a UTC ISO8601 timestamp, including the CODEX_ITERATION (7), task ID, and a test plus lint status summary.
exec
/bin/zsh -lc 'date -u +"%Y-%m-%dT%H:%M:%SZ"' in /Users/ivankhokhlov/code/mlx-ui succeeded in 71ms:
2026-01-21T12:03:50Z
file update:
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")


thinking
**Appending test and lint results**

I'm updating the progress file with the latest successful test and lint pass, including timestamp and iteration metadata.
exec
/bin/zsh -lc 'echo "- 2026-01-21T12:03:50Z — WORKER — CODEX_ITERATION=7 — WUI-012 — tests: passed (make test); lint: passed (make lint)" >> .agent/progress.md' in /Users/ivankhokhlov/code/mlx-ui succeeded in 70ms:

file update:
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")



thinking
**Confirming no extra changes**
thinking
**Outlining final response structure**

The plan is to open with a concise implementation explanation followed by detailed file change references, test results with log paths, and pointers to worker_report and logs, plus a natural next step suggestion, all without starting with "summary". I'll also ensure ASCII compliance concerns about em dashes are noted for the final content.
exec
/bin/zsh -lc 'cat .agent/worker_report.md' in /Users/ivankhokhlov/code/mlx-ui succeeded in 56ms:
# Worker Report

Task: WUI-012 — Sequential worker + fake transcriber

What changed:
- Added an atomic job-claim API plus status updates for queued/running/done/failed.
- Implemented a single background worker and fake transcriber that writes deterministic TXT output.
- Wired worker startup into app startup and added a sequential worker test.
- Updated existing app tests to keep the worker disabled during upload checks.
- Updated docs tree to reflect new modules/tests.

Files changed:
- mlx_ui/app.py
- mlx_ui/db.py
- mlx_ui/transcriber.py
- mlx_ui/worker.py
- tests/test_app.py
- tests/test_worker.py
- docs/tree.md

file update:
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")


thinking
**Standardizing dashes to ASCII**

I'm updating the worker_report file to replace the em dash with a hyphen to maintain ASCII-only content, while keeping em dashes in files that already use them like `.agent/progress.md`.
file update
M /Users/ivankhokhlov/code/mlx-ui/./.agent/worker_report.md
@@ -2,3 +2,3 @@
 
-Task: WUI-012 — Sequential worker + fake transcriber
+Task: WUI-012 - Sequential worker + fake transcriber
 
apply_patch(auto_approved=true) exited 0 in 69ms:
Success. Updated the following files:
M .agent/worker_report.md
file update:
diff --git a/.agent/worker_report.md b/.agent/worker_report.md
index 991c123165bd6b4bc965ab3500d8353cc2713dc7..b9318572f10debb0b3206755249e58492f3a3de3
--- a/.agent/worker_report.md
+++ b/.agent/worker_report.md
@@ -1,6 +1,6 @@
 # Worker Report
 
-Task: WUI-012 — Sequential worker + fake transcriber
+Task: WUI-012 - Sequential worker + fake transcriber
 
 What changed:
 - Added an atomic job-claim API plus status updates for queued/running/done/failed.
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")

file update:
diff --git a/.agent/worker_report.md b/.agent/worker_report.md
index 991c123165bd6b4bc965ab3500d8353cc2713dc7..b9318572f10debb0b3206755249e58492f3a3de3
--- a/.agent/worker_report.md
+++ b/.agent/worker_report.md
@@ -1,6 +1,6 @@
 # Worker Report
 
-Task: WUI-012 — Sequential worker + fake transcriber
+Task: WUI-012 - Sequential worker + fake transcriber
 
 What changed:
 - Added an atomic job-claim API plus status updates for queued/running/done/failed.
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")



thinking
**Verifying ASCII usage and worker logic**

Confirmed no new non-ASCII introduced; existing em dashes remain unchanged. All planned task updates including atomic claim, worker loop, fake transcriber, tests, and docs are done. Noted the worker start logic ignores new transcribers if already running, but tests handle this by stopping before restart.
thinking
**Confirming worker start/stop safety and test conditions**

Verified `start_worker` returns existing instance if running, and tests safely stop before restarts with adequate timeouts. The worker’s DB locking uses `BEGIN IMMEDIATE` to ensure atomic claims without concurrency issues. No new data directories or docs updates needed; import and logging usages are correct. Final repository state shows expected modified and new files.
exec
/bin/zsh -lc 'git status -sb' in /Users/ivankhokhlov/code/mlx-ui succeeded in 70ms:
## master
 M .agent/last_judge.txt
 M .agent/last_planner.txt
 M .agent/logs/judge_6.err
 M .agent/logs/judge_6.out
 M .agent/plan.md
 M .agent/progress.md
 M .agent/queue.md
 M .agent/worker_report.md
 M docs/tree.md
 M mlx_ui/app.py
 M mlx_ui/db.py
 M tests/test_app.py
?? .agent/logs/lint_7.log
?? .agent/logs/planner_7.err
?? .agent/logs/planner_7.out
?? .agent/logs/test_7.log
?? .agent/logs/worker_7.err
?? .agent/logs/worker_7.out
?? data/
file update:
diff --git a/.agent/worker_report.md b/.agent/worker_report.md
index 991c123165bd6b4bc965ab3500d8353cc2713dc7..b9318572f10debb0b3206755249e58492f3a3de3
--- a/.agent/worker_report.md
+++ b/.agent/worker_report.md
@@ -1,6 +1,6 @@
 # Worker Report
 
-Task: WUI-012 — Sequential worker + fake transcriber
+Task: WUI-012 - Sequential worker + fake transcriber
 
 What changed:
 - Added an atomic job-claim API plus status updates for queued/running/done/failed.
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")



thinking
**Noticing untracked data directory and harness files**

The untracked `data/` directory likely existed before or is created by app startup, but isn't from current tests since they use tmp paths. The `.agent/last_judge` and similar changed files come from the harness and should be ignored.
thinking
**Confirming handling of unrelated harness changes**

The observed modifications in `.agent/` files are likely harness-generated and pre-existing, so they aren't unexpected changes requiring user intervention. I'll proceed without stopping and include relevant test and worker report pointers in the final message as planned.
codex
Implemented a single background worker that atomically claims queued jobs and writes deterministic fake transcripts so processing stays strictly sequential and offline by default.
- Added atomic claim/update helpers in `mlx_ui/db.py` and a fake transcriber in `mlx_ui/transcriber.py`, wired into the loop in `mlx_ui/worker.py`.
- Started the worker on app startup with a test opt‑out in `mlx_ui/app.py`, and disabled it in `tests/test_app.py` to keep queued-status assertions stable.
- Added sequential worker coverage in `tests/test_worker.py` and updated `docs/tree.md`.

Tests: `make test` (pass; log `.agent/logs/test_7.log`), `make lint` (pass; log `.agent/logs/lint_7.log`).
Worker report: `.agent/worker_report.md`.

Next step: run `make run` and upload a couple files to see results under `data/results/<job_id>/result.txt`.
file update:
diff --git a/.agent/worker_report.md b/.agent/worker_report.md
index 991c123165bd6b4bc965ab3500d8353cc2713dc7..b9318572f10debb0b3206755249e58492f3a3de3
--- a/.agent/worker_report.md
+++ b/.agent/worker_report.md
@@ -1,6 +1,6 @@
 # Worker Report
 
-Task: WUI-012 — Sequential worker + fake transcriber
+Task: WUI-012 - Sequential worker + fake transcriber
 
 What changed:
 - Added an atomic job-claim API plus status updates for queued/running/done/failed.
diff --git a/docs/tree.md b/docs/tree.md
index ae097fb5a436817a904b215724378f37e52aa66a..b80b3c53d3b2148d4830121b7bfd9de2a390caec
--- a/docs/tree.md
+++ b/docs/tree.md
@@ -6,10 +6,10 @@
 - `.agent/` — durable loop state + prompts
 - `data/` — runtime uploads/results/logs/jobs.db (created on demand)
 - `docs/` — spec + dev notes + this tree map
-- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`)
+- `mlx_ui/` — FastAPI app package (`app.py`, `db.py`, `worker.py`, `transcriber.py`)
 - `mlx_ui/templates/` — Jinja2 templates
 - `scripts/` — loop scripts + setup/run scripts (to be implemented)
-- `tests/` — pytest suite
+- `tests/` — pytest suite (`test_app.py`, `test_worker.py`)
 - `Makefile` — dev commands
 - `pyproject.toml` — dependencies and tooling
 - `README.md` — repo overview
diff --git a/mlx_ui/app.py b/mlx_ui/app.py
index d2f6b05031a3edb4f5771e255f9f2f823a603e59..e18dd9889a4ce96fc96cd5ef7bd2ef082b84d27d
--- a/mlx_ui/app.py
+++ b/mlx_ui/app.py
@@ -8,19 +8,25 @@
 from fastapi.templating import Jinja2Templates
 
 from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker
 
 app = FastAPI(title="Whisper WebUI (MLX)")
 templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent / "templates"))
 BASE_DIR = Path(__file__).resolve().parent.parent
 DEFAULT_UPLOADS_DIR = BASE_DIR / "data" / "uploads"
+DEFAULT_RESULTS_DIR = BASE_DIR / "data" / "results"
 DEFAULT_DB_PATH = BASE_DIR / "data" / "jobs.db"
 app.state.uploads_dir = DEFAULT_UPLOADS_DIR
+app.state.results_dir = DEFAULT_RESULTS_DIR
 app.state.db_path = DEFAULT_DB_PATH
+app.state.worker_enabled = True
 
 
 @app.on_event("startup")
 def startup() -> None:
     init_db(get_db_path())
+    if getattr(app.state, "worker_enabled", True):
+        start_worker(get_db_path(), get_results_dir())
 
 
 def get_job_store() -> list[JobRecord]:
@@ -35,6 +41,10 @@
     return Path(app.state.uploads_dir)
 
 
+def get_results_dir() -> Path:
+    return Path(app.state.results_dir)
+
+
 def ensure_uploads_dir() -> Path:
     uploads_dir = get_uploads_dir()
     uploads_dir.mkdir(parents=True, exist_ok=True)
diff --git a/mlx_ui/db.py b/mlx_ui/db.py
index 00ae6b7f9df169c0aa0afb38710dafedf74ea5f2..e6fcd1c335c027ed1a91596da1a2ab4f1fbefa74
--- a/mlx_ui/db.py
+++ b/mlx_ui/db.py
@@ -59,3 +59,54 @@
         ).fetchall()
 
     return [JobRecord(**dict(row)) for row in rows]
+
+
+def update_job_status(db_path: Path, job_id: str, status: str) -> None:
+    with _connect(db_path) as connection:
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = ?
+            WHERE id = ?
+            """,
+            (status, job_id),
+        )
+        connection.commit()
+
+
+def claim_next_job(db_path: Path) -> JobRecord | None:
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    connection = sqlite3.connect(db_path, isolation_level=None)
+    connection.row_factory = sqlite3.Row
+    try:
+        connection.execute("BEGIN IMMEDIATE")
+        row = connection.execute(
+            """
+            SELECT id, filename, status, created_at, upload_path
+            FROM jobs
+            WHERE status = 'queued'
+            ORDER BY created_at ASC
+            LIMIT 1
+            """
+        ).fetchone()
+        if row is None:
+            connection.execute("COMMIT")
+            return None
+        job_id = row["id"]
+        connection.execute(
+            """
+            UPDATE jobs
+            SET status = 'running'
+            WHERE id = ?
+            """,
+            (job_id,),
+        )
+        connection.execute("COMMIT")
+        job_data = dict(row)
+        job_data["status"] = "running"
+        return JobRecord(**job_data)
+    except Exception:
+        connection.execute("ROLLBACK")
+        raise
+    finally:
+        connection.close()
diff --git a/mlx_ui/transcriber.py b/mlx_ui/transcriber.py
new file mode 100644
index 0000000000000000000000000000000000000000..3716a8e4021ce98aeae0fbbd3e4f7411ce26de56
--- /dev/null
+++ b/mlx_ui/transcriber.py
@@ -0,0 +1,20 @@
+from pathlib import Path
+from typing import Protocol
+
+from mlx_ui.db import JobRecord
+
+
+class Transcriber(Protocol):
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        raise NotImplementedError
+
+
+class FakeTranscriber:
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        results_dir = Path(results_dir)
+        job_dir = results_dir / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+        return result_path
diff --git a/mlx_ui/worker.py b/mlx_ui/worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..ce5d14a31f43e92b2beabf64d377b3f5a08481fc
--- /dev/null
+++ b/mlx_ui/worker.py
@@ -0,0 +1,97 @@
+from __future__ import annotations
+
+import logging
+from pathlib import Path
+import threading
+
+from mlx_ui.db import claim_next_job, update_job_status
+from mlx_ui.transcriber import FakeTranscriber, Transcriber
+
+logger = logging.getLogger(__name__)
+
+_worker_lock = threading.Lock()
+_worker_instance: Worker | None = None
+
+
+class Worker:
+    def __init__(
+        self,
+        db_path: Path,
+        results_dir: Path,
+        poll_interval: float = 0.5,
+        transcriber: Transcriber | None = None,
+    ) -> None:
+        self.db_path = Path(db_path)
+        self.results_dir = Path(results_dir)
+        self.poll_interval = poll_interval
+        self.transcriber = transcriber or FakeTranscriber()
+        self._stop_event = threading.Event()
+        self._thread: threading.Thread | None = None
+
+    def start(self) -> None:
+        if self.is_running():
+            return
+        self._stop_event.clear()
+        self._thread = threading.Thread(
+            target=self._run_loop,
+            name="mlx-ui-worker",
+            daemon=True,
+        )
+        self._thread.start()
+
+    def stop(self, timeout: float | None = None) -> None:
+        self._stop_event.set()
+        thread = self._thread
+        if thread is not None:
+            thread.join(timeout=timeout)
+
+    def is_running(self) -> bool:
+        return self._thread is not None and self._thread.is_alive()
+
+    def _run_loop(self) -> None:
+        while not self._stop_event.is_set():
+            processed = self.run_once()
+            if not processed:
+                self._stop_event.wait(self.poll_interval)
+
+    def run_once(self) -> bool:
+        job = claim_next_job(self.db_path)
+        if job is None:
+            return False
+        try:
+            self.transcriber.transcribe(job, self.results_dir)
+        except Exception:
+            logger.exception("Worker failed to transcribe job %s", job.id)
+            update_job_status(self.db_path, job.id, "failed")
+            return True
+        update_job_status(self.db_path, job.id, "done")
+        return True
+
+
+def start_worker(
+    db_path: Path,
+    results_dir: Path,
+    poll_interval: float = 0.5,
+    transcriber: Transcriber | None = None,
+) -> Worker:
+    global _worker_instance
+    with _worker_lock:
+        if _worker_instance and _worker_instance.is_running():
+            return _worker_instance
+        _worker_instance = Worker(
+            db_path=db_path,
+            results_dir=results_dir,
+            poll_interval=poll_interval,
+            transcriber=transcriber,
+        )
+        _worker_instance.start()
+        return _worker_instance
+
+
+def stop_worker(timeout: float | None = None) -> None:
+    global _worker_instance
+    with _worker_lock:
+        if not _worker_instance:
+            return
+        _worker_instance.stop(timeout=timeout)
+        _worker_instance = None
diff --git a/tests/test_app.py b/tests/test_app.py
index b767f47cf51c815672ab92e9d45d9de4060354eb..424ed321663b2a88c1574b6de2e4c46f0cd1ad12
--- a/tests/test_app.py
+++ b/tests/test_app.py
@@ -8,7 +8,9 @@
 
 def _configure_app(tmp_path: Path) -> None:
     app.state.uploads_dir = tmp_path / "uploads"
+    app.state.results_dir = tmp_path / "results"
     app.state.db_path = tmp_path / "jobs.db"
+    app.state.worker_enabled = False
 
 
 def test_root_ok(tmp_path: Path) -> None:
diff --git a/tests/test_worker.py b/tests/test_worker.py
new file mode 100644
index 0000000000000000000000000000000000000000..5aa32597e5833c7b595cdbe9aefee61056693369
--- /dev/null
+++ b/tests/test_worker.py
@@ -0,0 +1,102 @@
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+import threading
+import time
+
+from mlx_ui.db import JobRecord, init_db, insert_job, list_jobs
+from mlx_ui.worker import start_worker, stop_worker
+
+
+class RecordingTranscriber:
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        self._active = False
+        self.concurrent_detected = False
+        self.seen: list[str] = []
+
+    def transcribe(self, job: JobRecord, results_dir: Path) -> Path:
+        with self._lock:
+            if self._active:
+                self.concurrent_detected = True
+            self._active = True
+            self.seen.append(job.id)
+
+        time.sleep(0.05)
+
+        job_dir = Path(results_dir) / job.id
+        job_dir.mkdir(parents=True, exist_ok=True)
+        result_path = job_dir / "result.txt"
+        content = f"Fake transcript for {job.filename} ({job.id})\n"
+        result_path.write_text(content, encoding="utf-8")
+
+        with self._lock:
+            self._active = False
+        return result_path
+
+
+def _make_job(job_id: str, filename: str, created_at: str, uploads_dir: Path) -> JobRecord:
+    job_dir = uploads_dir / job_id
+    job_dir.mkdir(parents=True, exist_ok=True)
+    upload_path = job_dir / filename
+    upload_path.write_text("data", encoding="utf-8")
+    return JobRecord(
+        id=job_id,
+        filename=filename,
+        status="queued",
+        created_at=created_at,
+        upload_path=str(upload_path),
+    )
+
+
+def _wait_for_jobs(db_path: Path, expected_count: int, timeout: float = 2.0) -> list[JobRecord]:
+    deadline = time.monotonic() + timeout
+    while time.monotonic() < deadline:
+        jobs = list_jobs(db_path)
+        if len(jobs) == expected_count and all(job.status == "done" for job in jobs):
+            return jobs
+        time.sleep(0.01)
+    raise AssertionError("Timed out waiting for jobs to complete.")
+
+
+def test_worker_processes_jobs_sequentially(tmp_path: Path) -> None:
+    db_path = tmp_path / "jobs.db"
+    uploads_dir = tmp_path / "uploads"
+    results_dir = tmp_path / "results"
+    init_db(db_path)
+    uploads_dir.mkdir(parents=True, exist_ok=True)
+
+    base_time = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    job1 = _make_job(
+        "job1",
+        "alpha.txt",
+        base_time.isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    job2 = _make_job(
+        "job2",
+        "beta.txt",
+        (base_time + timedelta(seconds=1)).isoformat(timespec="seconds"),
+        uploads_dir,
+    )
+    insert_job(db_path, job1)
+    insert_job(db_path, job2)
+
+    transcriber = RecordingTranscriber()
+    start_worker(
+        db_path,
+        results_dir,
+        poll_interval=0.01,
+        transcriber=transcriber,
+    )
+    try:
+        jobs = _wait_for_jobs(db_path, expected_count=2)
+    finally:
+        stop_worker(timeout=1)
+
+    assert transcriber.concurrent_detected is False
+    assert transcriber.seen == [job1.id, job2.id]
+    for job in jobs:
+        result_path = results_dir / job.id / "result.txt"
+        assert result_path.is_file()
+        assert job.status == "done"
+        assert job.filename in result_path.read_text(encoding="utf-8")

tokens used
48,260
